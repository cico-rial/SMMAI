{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bade4d1e",
   "metadata": {},
   "source": [
    "# Homework 3\n",
    "### Optimization via Stochastic Gradient Descent.\n",
    "Here goes the text of the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62102320",
   "metadata": {},
   "source": [
    "The following has been written by me during classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3ca9a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_module.utility import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "63ffb66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset: D = (X, y) where X has shape (N,)\n",
    "# and y has shape(N,)\n",
    "\n",
    "# Have a function: f(w, X) and a function grad_f(w, X)\n",
    "\n",
    "#### TASK 1:\n",
    "# Implement functions loss(f, w, D) and grad_loss(f, grad_f, w, D)\n",
    "# that compute the loss (MSE) and its gradient\n",
    "\n",
    "def loss(f, w, D):\n",
    "    X, y = D\n",
    "    y_pred = f(w, X)\n",
    "    return np.mean(np.square(y_pred - y))\n",
    "\n",
    "def grad_loss(f, grad_f, w, D):\n",
    "    X, y = D\n",
    "    \n",
    "    return np.mean(2 * grad_f(w, X).T * (f(w, X) - y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c407514",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent\n",
    "You sample a batch of data, e.g. batch_size = 10 and you compute the values for batch_size instead of N.  \n",
    "Then you start again with another batch until you've finished your data. This is an **epoch**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e7aad05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK (HARD)\n",
    "# Implement a function: \n",
    "# SGD(loss, grad_loss, f, grad_f, D, batch_size, n_epochs) (you can also define f and grad_f globally)\n",
    "# that implements the Stochastic Gradient Descent algorithm with\n",
    "# given batch_size for the given number of epochs.\n",
    "# alpha <-> learning_rate FIXED at 1e-3 (it doesn't have stopping conditions)\n",
    "\n",
    "# Note: as what i've understood the learning rate cannot be automatically recomputed, \n",
    "# it has to be set by hand\n",
    "\n",
    "# f and grad_f could be defined globally\n",
    "def SGD(loss, grad_loss, w0, f, grad_f, D, batch_size, n_epochs, learning_rate=1e-3):\n",
    "    # Initialize the parameters w\n",
    "    w = w0\n",
    "    \n",
    "    # We assume that batch_size divides N\n",
    "    # Compute the number of batches per epoch\n",
    "    \n",
    "    n_batch_per_epoch = N // batch_size # integer division in case batch_size doesn't divide N\n",
    "    \n",
    "    # Extract X and y from D\n",
    "    X, y = D\n",
    "    \n",
    "    # Save in memory the length of X (and y)    They have the same dimension\n",
    "    N = X.shape[0]\n",
    "    d = w0.shape[0]\n",
    "    \n",
    "     # Initialize the output\n",
    "    \n",
    "    # a vector containing the value of l(w_k; D) after each epoch\n",
    "    loss_history = np.zeros((n_epochs,))\n",
    "    # a vector (w) containing the value of w_k for each iterate\n",
    "    w_history = np.zeros((n_epochs, d))\n",
    "    # a vector containing the value of grad_l(w_k; D) after each epoch\n",
    "    grad_loss_history = np.zeros((n_epochs,))\n",
    "    # a vector containing the value of ||grad_l(w_k; D)||_2 after each epoch\n",
    "    err = np.zeros((n_epochs,))\n",
    "    \n",
    "    # Iterate over the epochs\n",
    "    for epoch in range(n_epochs):\n",
    "        idx = np.arange(N)\n",
    "        np.random.shuffle(idx)\n",
    "        \n",
    "        # Batch iteration\n",
    "        for k in range(n_batch_per_epoch):\n",
    "            X_temp = X[idx[k * batch_size : (k+1) * batch_size]]\n",
    "            y_temp = y[idx[k * batch_size : (k+1) * batch_size]]\n",
    "            \n",
    "            B = (X_temp, y_temp)\n",
    "            \n",
    "            # Gradient descent update\n",
    "            # The gradient has to be computed in B, \n",
    "            # it cannot be computed on the entire dataset as it will crash!!\n",
    "            gradient = grad_loss(f, grad_f, w, B) \n",
    "            w = w - learning_rate * gradient\n",
    "        \n",
    "        w_history[epoch] = w \n",
    "        loss_history[epoch] = loss(f, w, B)  \n",
    "        grad_loss_history[epoch] = gradient\n",
    "        err[epoch] = np.linalg.norm(gradient)\n",
    "        \n",
    "        \n",
    "    return w, w_history, loss_history, grad_loss_history, err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9118117",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c2a62250",
   "metadata": {},
   "source": [
    "### End of the part written by me"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed927cf",
   "metadata": {},
   "source": [
    "# SGD  \n",
    "The implementation of the SGD algorithm according to **Davide Evangelista**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44f2e980",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def sgd_optimizer(loss, grad_loss, w0, D, alpha, batch_size, n_epochs):\n",
    "        X, y = D  # Unpack the data\n",
    "        N = X.shape[0]\n",
    "        d = w0.shape[0]\n",
    "        idx = np.arange(0, N)\n",
    "        \n",
    "        # Initialization of history vectors\n",
    "        w_history = np.zeros((n_epochs, d))  # Save weights at each iteration\n",
    "        loss_history = np.zeros((n_epochs, ))  # Save loss values at each iteration\n",
    "        grad_norm_history = np.zeros((n_epochs, ))  # Save gradient norms at each iteration\n",
    "        \n",
    "        # Initialize weights\n",
    "        w = w0\n",
    "        for epoch in range(n_epochs):\n",
    "            # Shuffle the data at the beginning of each epochn,\n",
    "            np.random.shuffle(idx)\n",
    "            X = X[idx]\n",
    "            y = y[idx]\n",
    "    \n",
    "            # Initialize a vector that saves the gradient of the loss at each iteration\n",
    "            grad_loss_vec = []\n",
    "            \n",
    "            for batch_start in range(0, N, batch_size):\n",
    "                batch_end = min(batch_start + batch_size, N)\n",
    "                X_batch = X[batch_startbatch_end]\n",
    "                y_batch = y[batch_startbatch_end]\n",
    "                \n",
    "                # Compute the gradient of the loss\n",
    "                gradient = grad_loss(w, X_batch, y_batch)\n",
    "                grad_loss_vec.append(np.linalg.norm(gradient, 2))\n",
    "    \n",
    "                # Update weights\n",
    "                w = w - alpha*gradient\n",
    "    \n",
    "            # Save the updated values\n",
    "            w_history[epoch] = w\n",
    "            loss_history[epoch] = loss(w, X, y)\n",
    "            grad_norm_history[epoch] = np.mean(grad_loss_vec)\n",
    "        \n",
    "        return w_history, loss_history, grad_norm_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ce2643f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56eb736b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
